{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13274518,"sourceType":"datasetVersion","datasetId":8412327},{"sourceId":13288999,"sourceType":"datasetVersion","datasetId":8422245},{"sourceId":600490,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":449927,"modelId":466298},{"sourceId":692548,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":525108,"modelId":539145},{"sourceId":694318,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":526531,"modelId":540577}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install dicom2nifti\n!pip install monai nibabel torch torchvision tqdm","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\n\nimport glob, shutil, tempfile\nimport dicom2nifti\nimport dicom2nifti.settings as settings\nimport torch\nimport nibabel as nib\nfrom monai.transforms import (\n    LoadImage,\n    EnsureChannelFirst,\n    Spacing,\n    Orientation,\n    ScaleIntensity,\n    Resize,\n    ToTensor\n)\nfrom monai.networks.nets import resnet\nfrom tqdm import tqdm\nimport torch.nn.functional as F\n\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport nibabel as nib","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# To convert DICOM files into NifTi","metadata":{}},{"cell_type":"code","source":"DATASET_DIR = \"/kaggle/input/data-ad/Dataset\"   \nOUTPUT_DIR  = \"/kaggle/working/NIFTI_DATA99\" \nos.makedirs(OUTPUT_DIR, exist_ok=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"settings.disable_validate_orthogonal() ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def convert_subject(dicom_dir, out_dir, sub_id):\n    \n    with tempfile.TemporaryDirectory() as tmp_out:\n        \n        dicom2nifti.convert_directory(dicom_dir, tmp_out, compression=True, reorient=True)\n\n        \n        nii_files = glob.glob(os.path.join(tmp_out, \"*.nii.gz\"))\n        src_file = nii_files[0]\n        dst_file = os.path.join(out_dir, f\"{sub_id}.nii.gz\")\n        shutil.move(src_file, dst_file)\n        # print(f\"{sub_id} to {dst_file}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for cls in os.listdir(DATASET_DIR):  \n    cls_path = os.path.join(DATASET_DIR, cls)\n    if not os.path.isdir(cls_path):\n        continue\n\n    out_cls_dir = os.path.join(OUTPUT_DIR, cls)\n    os.makedirs(out_cls_dir, exist_ok=True)\n\n    for subj in os.listdir(cls_path):\n        subj_path = os.path.join(cls_path, subj)\n        if not os.path.isdir(subj_path):\n            continue\n\n        dicom_files = glob.glob(os.path.join(subj_path, \"**\", \"*.dcm\"), recursive=True)\n\n        dicom_dir = os.path.dirname(dicom_files[0])\n        convert_subject(dicom_dir, out_cls_dir, subj)","metadata":{"trusted":true,"_kg_hide-output":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!ls /kaggle/working/NIFTI_DATA/ADNI1_T1w_Cohort_AD_Visit_12_MRI | wc -l","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!zip -r NIFTI_DATA99.zip /kaggle/working/NIFTI_DATA99\n","metadata":{"trusted":true,"_kg_hide-output":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Now will use converted nifti files to finetune and extract the MRI img","metadata":{}},{"cell_type":"code","source":"DATA_DIR = \"/kaggle/input/nifti-data99/kaggle/working/NIFTI_DATA99\"\nEMBEDDING_SAVE_DIR = \"/kaggle/working/MRI_Embeddings\"\nos.makedirs(EMBEDDING_SAVE_DIR, exist_ok=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.cuda.amp import GradScaler, autocast\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"num_classes = 3  \nmodel = resnet.resnet10(spatial_dims=3, n_input_channels=1, num_classes=1000)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from collections import OrderedDict\ncheckpoint = torch.load(\"/kaggle/input/resnet-10-23/pytorch/default/1/resnet_10_23dataset.pth\", map_location=device)\nstate_dict = checkpoint['state_dict'] \n\nnew_state_dict = OrderedDict()\nfor k, v in state_dict.items():\n    name = k.replace(\"module.\", \"\") \n    new_state_dict[name] = v\n\nmodel.load_state_dict(new_state_dict, strict=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"in_features = model.fc.in_features\nmodel.fc = nn.Sequential(\n    nn.Dropout(0.2),\n    nn.Linear(in_features, num_classes)\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(model.conv1.weight.shape)\nprint(new_state_dict['conv1.weight'].shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"in_features","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.fc","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if torch.cuda.device_count() > 1:\n    print(f\"Using {torch.cuda.device_count()} GPUs!\")\n    model = nn.DataParallel(model)\n\nmodel = model.to(device)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for param in model.parameters():\n    param.requires_grad = True","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"unfreeze_layers = [\"layer4\", \"fc\"]\nfor name, param in model.named_parameters():\n    if any(l in name for l in unfreeze_layers):\n        param.requires_grad = True\n    else:\n        param.requires_grad = False\n        ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainable_params = 0\ntotal_params = 0\n\nfor name, param in model.named_parameters():\n    total_params += param.numel()\n    if param.requires_grad:\n        trainable_params += param.numel()\n        if \"conv1\" in name or \".0.conv1\" in name or \"fc\" in name:\n            print(f\"TRAINABLE: {name}\")\n    else:\n        pass \n\nprint(f\"{trainable_params:,} trainable parameters out of {total_params:,} total.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = model.to(device)\n\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0)\n#optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=3e-4, weight_decay=0)\ncriterion = nn.CrossEntropyLoss()\nscaler = torch.amp.GradScaler()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"num_trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\nnum_total = sum(p.numel() for p in model.parameters())\n\nprint(f\"Trainable params: {num_trainable}\")\nprint(f\"Total params: {num_total}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader\nimport gc","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from monai.transforms import (\n    Compose,\n    LoadImaged,\n    EnsureChannelFirstd,\n    ScaleIntensityd,\n    NormalizeIntensityd,\n    Resized,\n    EnsureTyped\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from monai.transforms import Spacingd, Orientationd\npreprocess = Compose([\n    LoadImaged(keys=[\"image\"]),\n    EnsureChannelFirstd(keys=[\"image\"]),\n    Spacingd(keys=[\"image\"], pixdim=(1.0,1.0,1.0), mode=\"bilinear\"),\n    Orientationd(keys=[\"image\"], axcodes=\"RAS\"),\n    NormalizeIntensityd(keys=[\"image\"], nonzero=True, channel_wise=True),\n    Resized(keys=[\"image\"], spatial_size=(128,128,128)),\n    EnsureTyped(keys=[\"image\"]),\n])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import train_test_split","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class MRIDataset(Dataset):\n    def __init__(self, files, transform):\n        self.files = files\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.files)\n\n    def __getitem__(self, idx):\n        data = dict(self.files[idx])\n        data = self.transform(data)\n\n        image = data[\"image\"]\n        label = data.get(\"label\", None)\n\n        if not torch.is_tensor(label):\n            label = torch.tensor(label, dtype=torch.long)\n        else:\n            label = label.long()\n\n        return image, label\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"all_files = []\nclass_to_idx = {\n    \"ADNI1_T1w_Cohort_AD_Visit_12_MRI\": 0, \n    \"ADNI1_T1w_DXMCI=1_at_m18_MRI\": 1, \n    \"ADNI1_T1w_Normal_at_m12_MRI\": 2\n}\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for cls_name, idx in class_to_idx.items():\n    cls_path = os.path.join(DATA_DIR, cls_name)\n    if os.path.exists(cls_path):\n        for img in os.listdir(cls_path):\n            if img.endswith('.nii') or img.endswith('.nii.gz'):\n                all_files.append({\"image\": os.path.join(cls_path, img), \"label\": idx})","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_files, val_files = train_test_split(all_files, test_size=0.25, stratify=[x['label'] for x in all_files], random_state=42)\n\ntrain_ds = MRIDataset(train_files, transform=preprocess)\nval_ds = MRIDataset(val_files, transform=preprocess)\n\ntrain_loader = DataLoader(train_ds, batch_size=20, shuffle=True, num_workers=4, pin_memory=True, drop_last=True)\nval_loader = DataLoader(val_ds, batch_size=20, shuffle=False, num_workers=4, pin_memory=True)\n\nprint(len(train_ds))\nprint(len(val_ds))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"batch = next(iter(train_loader))\nimages, labels = batch\nprint(\"images.shape\", images.shape)\nprint(\"images.dtype\", images.dtype, \"min/max/mean/std:\", images.min().item(), images.max().item(), images.mean().item(), images.std().item())\nprint(\"labels.shape\", labels.shape, \"labels.dtype\", labels.dtype, \"unique:\", torch.unique(labels))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch.cuda.empty_cache()\ngc.collect()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Test for overfitting through 1 batch test\nimages, labels = next(iter(train_loader))\nimages, labels = images.to(device), labels.to(device)\n\nmodel.train()\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0) \n\nfor i in range(50): \n    optimizer.zero_grad()\n    with torch.cuda.amp.autocast():\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n    \n    scaler.scale(loss).backward()\n    scaler.step(optimizer)\n    scaler.update()\n    \n    if i % 10 == 0:\n        _, preds = torch.max(outputs, 1)\n        acc = (preds == labels).sum().item() / labels.size(0)\n        print(f\"Step {i}: Loss {loss.item():.4f} | Acc: {acc*100:.1f}%\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"EPOCHS = 50\nbest_metric = -1\n\nfor epoch in range(EPOCHS):\n    model.train() \n    train_loss, train_correct, train_total = 0, 0, 0\n    \n    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1} Training\"):\n        images, labels = batch\n        images, labels = images.to(device), labels.to(device)\n        \n        optimizer.zero_grad()\n        with autocast(): \n            outputs = model(images)\n            loss = criterion(outputs, labels)\n        \n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n        \n        train_loss += loss.item()\n        _, predicted = torch.max(outputs, 1)\n        train_total += labels.size(0)\n        train_correct += (predicted == labels).sum().item()\n\n\n    model.eval() \n    val_loss, val_correct, val_total = 0, 0, 0\n    \n    with torch.no_grad():\n        for batch in tqdm(val_loader, desc=f\"Epoch {epoch+1} Validation\"):\n            images, labels = batch\n            images, labels = images.to(device), labels.to(device)\n            \n            with autocast():\n                outputs = model(images)\n                loss = criterion(outputs, labels)\n            \n            val_loss += loss.item()\n            _, predicted = torch.max(outputs, 1)\n            val_total += labels.size(0)\n            val_correct += (predicted == labels).sum().item()\n    \n\n    train_acc = 100 * train_correct / train_total\n    val_acc = 100 * val_correct / val_total\n    \n    print(f\"Epoch {epoch+1}: Train Acc: {train_acc:.2f}% | Val Acc: {val_acc:.2f}%\")\n    \n \n    if val_acc > best_metric:\n        best_metric = val_acc\n        torch.save(model.module.state_dict(), \"best_model.pth\")\n        print(\"New Best Saved\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# fOR SINGLE BATCH\ndata_iter = iter(train_loader)\nimg, lbl = next(data_iter)\n\nprint(f\"Img Batch Shape: {img.shape}\")\nprint(f\"Lbl Batch: {lbl}\")\nprint(f\"Img Max Value: {img.max().item()}\")\nprint(f\"Img Min Value: {img.min().item()}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#torch.save(model.state_dict(), \"finetuned_medicalnet_resnet50_pro.pth\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Finetuned model will be used for extracting embeddings ","metadata":{}},{"cell_type":"code","source":"model = resnet.resnet10(spatial_dims=3, n_input_channels=1, num_classes=3)\nstate_dict = torch.load(\"/kaggle/working/best_metric_model.pth\", map_location=device)\nmodel.load_state_dict(state_dict, strict=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = model.to(device)\nmodel.eval()\n\nfeature_extractor = nn.Sequential(*list(model.children())[:-1])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def extract_embedding(img_tensor):\n    with torch.no_grad():\n        feat = feature_extractor(img_tensor_5d)\n        feat = feat.view(feat.size(0), -1)              \n    return feat.cpu().numpy().squeeze()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for cls in os.listdir(DATA_DIR):\n    cls_dir = os.path.join(DATA_DIR, cls)\n    if not os.path.isdir(cls_dir):\n        continue\n\n    save_cls_dir = os.path.join(EMBEDDING_SAVE_DIR, cls)\n    os.makedirs(save_cls_dir, exist_ok=True)\n\n    nii_files = [f for f in os.listdir(cls_dir) if f.endswith((\".nii\", \".nii.gz\"))]\n\n    for file in tqdm(nii_files, desc=f\"Processing {cls}\"):\n        \n        img_path = os.path.join(cls_dir, file)\n        img = nib.load(img_path).get_fdata()\n        img = np.nan_to_num(img)\n\n        #if img.max() > 0:\n        #    img = img / img.max()\n        \n        img_tensor = torch.tensor(img, dtype=torch.float32).unsqueeze(0).unsqueeze(0)  \n        \n        img_tensor = img_tensor.to(device)\n        img_tensor = F.interpolate(img_tensor, size=(128, 128, 128), mode=\"trilinear\", align_corners=False)\n\n     \n        embedding = extract_embedding(img_tensor)\n\n    \n        if file.endswith(\".nii.gz\"):\n            subj_id = file[:-7]\n        elif file.endswith(\".nii\"):\n            subj_id = file[:-4]\n        else:\n            subj_id = os.path.splitext(file)[0]\n\n        np.save(os.path.join(save_cls_dir, f\"{subj_id}_embedding.npy\"), embedding)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"MRI_DIR = \"/kaggle/working/MRI_Embeddings\"\nclasses = [\"ADNI1_T1w_Cohort_AD_Visit_12_MRI\", \"ADNI1_T1w_DXMCI=1_at_m18_MRI\", \"ADNI1_T1w_Normal_at_m12_MRI\"]\nrecords = []","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"MRI_DIR","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for cls in classes:\n    cls_path = os.path.join(MRI_DIR, cls)\n    for f in os.listdir(cls_path):\n        if f.endswith(\".npy\"):\n            subj_id = os.path.splitext(f)[0]\n            emb = np.load(os.path.join(cls_path, f))\n            records.append({\"subject_id\": subj_id, \"label\": cls, \"embedding\": emb})","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"mri_df = pd.DataFrame(records)\nmri_df","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_mri = np.stack([x.flatten() for x in mri_df['embedding']])\ny = mri_df['label']\nsubjects = mri_df['subject_id']","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(X_mri.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler, RobustScaler\n\nscaler = RobustScaler()\nX_mri= scaler.fit_transform(X_mri)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.manifold import TSNE\nimport seaborn as sns\n\n\ntsne = TSNE(n_components=2, perplexity=30, random_state=42, init='pca', learning_rate='auto')\nX_tsne = tsne.fit_transform(X_mri)\n\nplt.figure(figsize=(8, 6))\nsns.scatterplot(x=X_tsne[:,0], y=X_tsne[:,1], hue=y, palette=\"deep\", s=60)\nplt.title(\"MRI Embedding\")\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, accuracy_score","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X_mri, y, test_size=0.2, random_state=42, stratify=y)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"rf = RandomForestClassifier(n_estimators=400, random_state=42)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"rf.fit(X_train, y_train)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_pred = rf.predict(X_test)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"MRI accuracy:\", accuracy_score(y_test, y_pred))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!zip -r MRI_Embeddings.zip /kaggle/working/MRI_Embeddings","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}