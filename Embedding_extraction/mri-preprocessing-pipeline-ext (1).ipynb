{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install dicom2nifti\n",
    "!pip install monai nibabel torch torchvision tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import glob, shutil, tempfile\n",
    "import dicom2nifti\n",
    "import dicom2nifti.settings as settings\n",
    "import torch\n",
    "import nibabel as nib\n",
    "from monai.transforms import (\n",
    "    LoadImage,\n",
    "    EnsureChannelFirst,\n",
    "    Spacing,\n",
    "    Orientation,\n",
    "    ScaleIntensity,\n",
    "    Resize,\n",
    "    ToTensor\n",
    ")\n",
    "from monai.networks.nets import resnet\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import nibabel as nib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To convert DICOM files into NifTi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "DATASET_DIR = \"/kaggle/input/data-ad/Dataset\"   \n",
    "OUTPUT_DIR  = \"/kaggle/working/NIFTI_DATA99\" \n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "settings.disable_validate_orthogonal() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def convert_subject(dicom_dir, out_dir, sub_id):\n",
    "    \n",
    "    with tempfile.TemporaryDirectory() as tmp_out:\n",
    "        \n",
    "        dicom2nifti.convert_directory(dicom_dir, tmp_out, compression=True, reorient=True)\n",
    "\n",
    "        \n",
    "        nii_files = glob.glob(os.path.join(tmp_out, \"*.nii.gz\"))\n",
    "        src_file = nii_files[0]\n",
    "        dst_file = os.path.join(out_dir, f\"{sub_id}.nii.gz\")\n",
    "        shutil.move(src_file, dst_file)\n",
    "        # print(f\"{sub_id} to {dst_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "for cls in os.listdir(DATASET_DIR):  \n",
    "    cls_path = os.path.join(DATASET_DIR, cls)\n",
    "    if not os.path.isdir(cls_path):\n",
    "        continue\n",
    "\n",
    "    out_cls_dir = os.path.join(OUTPUT_DIR, cls)\n",
    "    os.makedirs(out_cls_dir, exist_ok=True)\n",
    "\n",
    "    for subj in os.listdir(cls_path):\n",
    "        subj_path = os.path.join(cls_path, subj)\n",
    "        if not os.path.isdir(subj_path):\n",
    "            continue\n",
    "\n",
    "        dicom_files = glob.glob(os.path.join(subj_path, \"**\", \"*.dcm\"), recursive=True)\n",
    "\n",
    "        dicom_dir = os.path.dirname(dicom_files[0])\n",
    "        convert_subject(dicom_dir, out_cls_dir, subj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!ls /kaggle/working/NIFTI_DATA/ADNI1_T1w_Cohort_AD_Visit_12_MRI | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!zip -r NIFTI_DATA99.zip /kaggle/working/NIFTI_DATA99\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now will use converted nifti files to finetune and extract the MRI img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "DATA_DIR = \"/kaggle/input/nifti-data99/kaggle/working/NIFTI_DATA99\"\n",
    "EMBEDDING_SAVE_DIR = \"/kaggle/working/MRI_Embeddings\"\n",
    "os.makedirs(EMBEDDING_SAVE_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torch.cuda.amp import GradScaler, autocast\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "num_classes = 3  \n",
    "model = resnet.resnet10(spatial_dims=3, n_input_channels=1, num_classes=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "checkpoint = torch.load(\"/kaggle/input/resnet-10-23/pytorch/default/1/resnet_10_23dataset.pth\", map_location=device)\n",
    "state_dict = checkpoint['state_dict'] \n",
    "\n",
    "new_state_dict = OrderedDict()\n",
    "for k, v in state_dict.items():\n",
    "    name = k.replace(\"module.\", \"\") \n",
    "    new_state_dict[name] = v\n",
    "\n",
    "model.load_state_dict(new_state_dict, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "in_features = model.fc.in_features\n",
    "model.fc = nn.Sequential(\n",
    "    nn.Dropout(0.2),\n",
    "    nn.Linear(in_features, num_classes)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(model.conv1.weight.shape)\n",
    "print(new_state_dict['conv1.weight'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "in_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model.fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs!\")\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "unfreeze_layers = [\"layer4\", \"fc\"]\n",
    "for name, param in model.named_parameters():\n",
    "    if any(l in name for l in unfreeze_layers):\n",
    "        param.requires_grad = True\n",
    "    else:\n",
    "        param.requires_grad = False\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "trainable_params = 0\n",
    "total_params = 0\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    total_params += param.numel()\n",
    "    if param.requires_grad:\n",
    "        trainable_params += param.numel()\n",
    "        if \"conv1\" in name or \".0.conv1\" in name or \"fc\" in name:\n",
    "            print(f\"TRAIN: {name}\")\n",
    "    else:\n",
    "        pass \n",
    "\n",
    "print(f\"{trainable_params:,} of {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0)\n",
    "#optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=3e-4, weight_decay=0)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "scaler = torch.amp.GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from monai.transforms import (\n",
    "    Compose,\n",
    "    LoadImaged,\n",
    "    EnsureChannelFirstd,\n",
    "    ScaleIntensityd,\n",
    "    NormalizeIntensityd,\n",
    "    Resized,\n",
    "    EnsureTyped\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from monai.transforms import Spacingd, Orientationd\n",
    "preprocess = Compose([\n",
    "    LoadImaged(keys=[\"image\"]),\n",
    "    EnsureChannelFirstd(keys=[\"image\"]),\n",
    "    Spacingd(keys=[\"image\"], pixdim=(1.0,1.0,1.0), mode=\"bilinear\"),\n",
    "    Orientationd(keys=[\"image\"], axcodes=\"RAS\"),\n",
    "    NormalizeIntensityd(keys=[\"image\"], nonzero=True, channel_wise=True),\n",
    "    Resized(keys=[\"image\"], spatial_size=(128,128,128)),\n",
    "    EnsureTyped(keys=[\"image\"]),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class MRIDataset(Dataset):\n",
    "    def __init__(self, files, transform):\n",
    "        self.files = files\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = dict(self.files[idx])\n",
    "        data = self.transform(data)\n",
    "\n",
    "        image = data[\"image\"]\n",
    "        label = data.get(\"label\", None)\n",
    "\n",
    "        if not torch.is_tensor(label):\n",
    "            label = torch.tensor(label, dtype=torch.long)\n",
    "        else:\n",
    "            label = label.long()\n",
    "\n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "all_files = []\n",
    "class_to_idx = {\n",
    "    \"ADNI1_T1w_Cohort_AD_Visit_12_MRI\": 0, \n",
    "    \"ADNI1_T1w_DXMCI=1_at_m18_MRI\": 1, \n",
    "    \"ADNI1_T1w_Normal_at_m12_MRI\": 2\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "for cls_name, idx in class_to_idx.items():\n",
    "    cls_path = os.path.join(DATA_DIR, cls_name)\n",
    "    if os.path.exists(cls_path):\n",
    "        for img in os.listdir(cls_path):\n",
    "            if img.endswith('.nii') or img.endswith('.nii.gz'):\n",
    "                all_files.append({\"image\": os.path.join(cls_path, img), \"label\": idx})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_files, val_files = train_test_split(all_files, test_size=0.25, stratify=[x['label'] for x in all_files], random_state=42)\n",
    "\n",
    "train_ds = MRIDataset(train_files, transform=preprocess)\n",
    "val_ds = MRIDataset(val_files, transform=preprocess)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=20, shuffle=True, num_workers=4, pin_memory=True, drop_last=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=20, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "print(len(train_ds))\n",
    "print(len(val_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "batch = next(iter(train_loader))\n",
    "images, labels = batch\n",
    "print(\"images.shape\", images.shape)\n",
    "print(\"images.dtype\", images.dtype, \"min/max/mean/std:\", images.min().item(), images.max().item(), images.mean().item(), images.std().item())\n",
    "print(\"labels.shape\", labels.shape, \"labels.dtype\", labels.dtype, \"unique:\", torch.unique(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Test for overfitting through 1 batch test\n",
    "images, labels = next(iter(train_loader))\n",
    "images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "model.train()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0) \n",
    "\n",
    "for i in range(50): \n",
    "    optimizer.zero_grad()\n",
    "    with torch.cuda.amp.autocast():\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "    \n",
    "    scaler.scale(loss).backward()\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "    \n",
    "    if i % 10 == 0:\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        acc = (preds == labels).sum().item() / labels.size(0)\n",
    "        print(f\"Step {i}: Loss {loss.item():.4f} | Acc: {acc*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "best_metric = -1\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train() \n",
    "    train_loss, train_correct, train_total = 0, 0, 0\n",
    "    \n",
    "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1} Training\"):\n",
    "        images, labels = batch\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        with autocast(): \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        train_total += labels.size(0)\n",
    "        train_correct += (predicted == labels).sum().item()\n",
    "\n",
    "\n",
    "    model.eval() \n",
    "    val_loss, val_correct, val_total = 0, 0, 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=f\"Epoch {epoch+1} Validation\"):\n",
    "            images, labels = batch\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            with autocast():\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            val_total += labels.size(0)\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "    \n",
    "\n",
    "    train_acc = 100 * train_correct / train_total\n",
    "    val_acc = 100 * val_correct / val_total\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}: Train Acc: {train_acc:.2f}% | Val Acc: {val_acc:.2f}%\")\n",
    "    \n",
    " \n",
    "    if val_acc > best_metric:\n",
    "        best_metric = val_acc\n",
    "        torch.save(model.module.state_dict(), \"best_model.pth\")\n",
    "        print(\"New Best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# fOR SINGLE BATCH\n",
    "data_iter = iter(train_loader)\n",
    "img, lbl = next(data_iter)\n",
    "\n",
    "print(f\"Img Batch Shape: {img.shape}\")\n",
    "print(f\"Lbl Batch: {lbl}\")\n",
    "print(f\"Img Max Value: {img.max().item()}\")\n",
    "print(f\"Img Min Value: {img.min().item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#torch.save(model.state_dict(), \"finetuned_medicalnet_resnet50_pro.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finetuned model will be used for extracting embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model = resnet.resnet10(spatial_dims=3, n_input_channels=1, num_classes=3)\n",
    "state_dict = torch.load(\"/kaggle/working/best_model.pth\", map_location=device)\n",
    "model.load_state_dict(state_dict, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "feature_extractor = nn.Sequential(*list(model.children())[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def extract_embedding(img_tensor):\n",
    "    with torch.no_grad():\n",
    "        feat = feature_extractor(img_tensor_5d)\n",
    "        feat = feat.view(feat.size(0), -1)              \n",
    "    return feat.cpu().numpy().squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "for cls in os.listdir(DATA_DIR):\n",
    "    cls_dir = os.path.join(DATA_DIR, cls)\n",
    "    if not os.path.isdir(cls_dir):\n",
    "        continue\n",
    "\n",
    "    save_cls_dir = os.path.join(EMBEDDING_SAVE_DIR, cls)\n",
    "    os.makedirs(save_cls_dir, exist_ok=True)\n",
    "\n",
    "    nii_files = [f for f in os.listdir(cls_dir) if f.endswith((\".nii\", \".nii.gz\"))]\n",
    "\n",
    "    for file in tqdm(nii_files, desc=f\"Processing {cls}\"):\n",
    "        \n",
    "        img_path = os.path.join(cls_dir, file)\n",
    "        img = nib.load(img_path).get_fdata()\n",
    "        img = np.nan_to_num(img)\n",
    "\n",
    "        #if img.max() > 0:\n",
    "        #    img = img / img.max()\n",
    "        \n",
    "        img_tensor = torch.tensor(img, dtype=torch.float32).unsqueeze(0).unsqueeze(0)  \n",
    "        \n",
    "        img_tensor = img_tensor.to(device)\n",
    "        img_tensor = F.interpolate(img_tensor, size=(128, 128, 128), mode=\"trilinear\", align_corners=False)\n",
    "\n",
    "     \n",
    "        embedding = extract_embedding(img_tensor)\n",
    "\n",
    "    \n",
    "        if file.endswith(\".nii.gz\"):\n",
    "            subj_id = file[:-7]\n",
    "        elif file.endswith(\".nii\"):\n",
    "            subj_id = file[:-4]\n",
    "        else:\n",
    "            subj_id = os.path.splitext(file)[0]\n",
    "\n",
    "        np.save(os.path.join(save_cls_dir, f\"{subj_id}_embedding.npy\"), embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "MRI_DIR = \"/kaggle/working/MRI_Embeddings\"\n",
    "classes = [\"ADNI1_T1w_Cohort_AD_Visit_12_MRI\", \"ADNI1_T1w_DXMCI=1_at_m18_MRI\", \"ADNI1_T1w_Normal_at_m12_MRI\"]\n",
    "records = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "MRI_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "for cls in classes:\n",
    "    cls_path = os.path.join(MRI_DIR, cls)\n",
    "    for f in os.listdir(cls_path):\n",
    "        if f.endswith(\".npy\"):\n",
    "            subj_id = os.path.splitext(f)[0]\n",
    "            emb = np.load(os.path.join(cls_path, f))\n",
    "            records.append({\"subject_id\": subj_id, \"label\": cls, \"embedding\": emb})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "mri_df = pd.DataFrame(records)\n",
    "mri_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "X_mri = np.stack([x.flatten() for x in mri_df['embedding']])\n",
    "y = mri_df['label']\n",
    "subjects = mri_df['subject_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(X_mri.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "\n",
    "scaler = RobustScaler()\n",
    "X_mri= scaler.fit_transform(X_mri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "tsne = TSNE(n_components=2, perplexity=30, random_state=42, init='pca', learning_rate='auto')\n",
    "X_tsne = tsne.fit_transform(X_mri)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x=X_tsne[:,0], y=X_tsne[:,1], hue=y, palette=\"deep\", s=60)\n",
    "plt.title(\"MRI Embedding\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_mri, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=400, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "y_pred = rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"MRI accuracy:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!zip -r MRI_Embeddings.zip /kaggle/working/MRI_Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8412327,
     "sourceId": 13274518,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8422245,
     "sourceId": 13288999,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 466298,
     "modelInstanceId": 449927,
     "sourceId": 600490,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 539145,
     "modelInstanceId": 525108,
     "sourceId": 692548,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 540577,
     "modelInstanceId": 526531,
     "sourceId": 694318,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
